{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find True North"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to determine the true north direction on two aerial photographs given their centerpoint geolocations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from geographiclib.geodesic import Geodesic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindNorth():\n",
    "\n",
    "    def detect_and_describe(self, image, alg = \"ORB\"):\n",
    "        \"\"\"\n",
    "        This function is modified from https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/\n",
    "        SIFT algorithm is proprietary and can only be used with opencv-python 3.4.2.17 and opencv-contrib-python 3.4.2.17\n",
    "        or earlier.  ORB is the default because it is fast and open source.\n",
    "        \n",
    "        inputs:\n",
    "        image: cv2.image object to have keypoints detected.\n",
    "        alg: which algorithm to use, can be \"SIFT\", \"ORB\", or \"BRISK\"\n",
    "        \n",
    "        outputs:\n",
    "        kps: keypoint locations\n",
    "        features: feature vectors of keypoints\n",
    "        \"\"\"\n",
    "        \n",
    "        if alg == \"SIFT\":\n",
    "            descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "        elif alg == \"ORB\":\n",
    "            descriptor = cv2.ORB_create()\n",
    "        elif alg == \"BRISK\":\n",
    "            descriptor = cv2.BRISK_create()\n",
    "        (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "        kps = np.float32([kp.pt for kp in kps])\n",
    "\n",
    "        return kps, features\n",
    "    \n",
    "    def match_keypoints(self, kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh):\n",
    "        \"\"\"\n",
    "        Modified from https://www.pyimagesearch.com/2016/01/11/opencv-panorama-stitching/.\n",
    "        returns the matching keypoints and homography matrix based on feature vectors returned from detect_and_describe.\n",
    "        \n",
    "        inputs:\n",
    "        kpsA: list keypoints from image A\n",
    "        kpsB: list keypoints from image B\n",
    "        featuresA: features from image A\n",
    "        featuresB: features from image B\n",
    "        ratio: lowe's ratio test ratio\n",
    "        reprojThresh: threshold number of keypoint matches for reprojection to occur\n",
    "        \n",
    "        outputs:\n",
    "        matches: list of matching keypoints\n",
    "        H: homography matrix\n",
    "        status: status of homography matrix search\n",
    "        \"\"\"\n",
    "        # compute the raw matches and initialize the list of actual matches\n",
    "        matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "        rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "        matches = []\n",
    "\n",
    "        # loop over the raw matches\n",
    "        for m in rawMatches:\n",
    "            # ensure the distance is within a certain ratio of each other (i.e. Lowe's ratio test)\n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "        # computing a homography requires at least 4 matches\n",
    "        if len(matches) > 4:\n",
    "            # construct the two sets of points\n",
    "            ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "            ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "\n",
    "            # compute the homography between the two sets of points\n",
    "            (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "\n",
    "            # return the matches along with the homograpy matrix and status of each matched point\n",
    "            return (matches, H, status)\n",
    "\n",
    "        # otherwise, no homograpy could be computed\n",
    "        return None\n",
    "\n",
    "        \n",
    "    def procrustes(self, X, Y, scaling=True, reflection=False):\n",
    "        \"\"\"\n",
    "        Modified from https://stackoverflow.com/questions/18925181/procrustes-analysis-with-numpy\n",
    "        A port of MATLAB's `procrustes` function to Numpy.\n",
    "\n",
    "        Procrustes analysis determines a linear transformation (translation,\n",
    "        reflection, orthogonal rotation and scaling) of the points in Y to best\n",
    "        conform them to the points in matrix X, using the sum of squared errors\n",
    "        as the goodness of fit criterion.\n",
    "\n",
    "        inputs:\n",
    "        X: matrix of input coordinates\n",
    "        Y: matrix of target coordinates.  Must have equal number of points (rows) to X, but may have \n",
    "        fewer dimensions than X\n",
    "        scaling: if False, the scaling component of the transformation is forced to 1.\n",
    "        reflection: if 'best' (default), the transformation solution may or may not include a reflection \n",
    "        component, depending on which fits the data best. setting reflection to True or False forces a \n",
    "        solution with reflection or no reflection respectively.\n",
    "\n",
    "        outputs\n",
    "        tform: a dict specifying the rotation, translation and scaling that maps X --> Y\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        n,m = X.shape\n",
    "        ny,my = Y.shape\n",
    "        muX = X.mean(0)\n",
    "        muY = Y.mean(0)\n",
    "        X0 = X - muX\n",
    "        Y0 = Y - muY\n",
    "        ssX = (X0**2.).sum()\n",
    "        ssY = (Y0**2.).sum()\n",
    "\n",
    "        # centred Frobenius norm\n",
    "        normX = np.sqrt(ssX)\n",
    "        normY = np.sqrt(ssY)\n",
    "        # scale to equal (unit) norm\n",
    "        X0 /= normX\n",
    "        Y0 /= normY\n",
    "\n",
    "        if my < m:\n",
    "            Y0 = np.concatenate((Y0, np.zeros(n, m-my)),0)\n",
    "\n",
    "        # optimum rotation matrix of Y\n",
    "        A = np.dot(X0.T, Y0)\n",
    "        U,s,Vt = np.linalg.svd(A,full_matrices=False)\n",
    "        V = Vt.T\n",
    "        T = np.dot(V, U.T)\n",
    "\n",
    "        if reflection is not 'best':\n",
    "            # does the current solution use a reflection?\n",
    "            have_reflection = np.linalg.det(T) < 0\n",
    "            # if that's not what was specified, force another reflection\n",
    "            if reflection != have_reflection:\n",
    "                V[:,-1] *= -1\n",
    "                s[-1] *= -1\n",
    "                T = np.dot(V, U.T)\n",
    "\n",
    "        traceTA = s.sum()\n",
    "\n",
    "        if scaling:\n",
    "            # optimum scaling of Y\n",
    "            b = traceTA * normX / normY\n",
    "            # standarised distance between X and b*Y*T + c\n",
    "            d = 1 - traceTA**2\n",
    "            # transformed coords\n",
    "            Z = normX*traceTA*np.dot(Y0, T) + muX\n",
    "        else:\n",
    "            b = 1\n",
    "            d = 1 + ssY/ssX - 2 * traceTA * normY / normX\n",
    "            Z = normY*np.dot(Y0, T) + muX\n",
    "\n",
    "        # transformation matrix\n",
    "        if my < m:\n",
    "            T = T[:my,:]\n",
    "        c = muX - b*np.dot(muY, T)\n",
    "        #transformation values \n",
    "        tform = {'rotation':T, 'scale':b, 'translation':c}\n",
    "\n",
    "        return tform\n",
    "    \n",
    "    def match_2_images(self, kpsA, kpsB, featuresA, featuresB, num_matches, ratio, reprojThresh):\n",
    "        \"\"\"\n",
    "        Returns the results for procrustes from the keypoints and features of two images.\n",
    "        \n",
    "        inputs:\n",
    "        kpsA: list keypoint locations of image A\n",
    "        kpsB: list keypoint locations of image B\n",
    "        featuresA: list feature vectors describimg kpsA from imageA\n",
    "        featuresB: list feature vectors describing kpsB from imageB\n",
    "        num_matches: int number of to be considerded for tform calculation\n",
    "        ratio: ratio for lowe's ratio test\n",
    "        reprojThresh: threshold number of keypoint matches for reprojection to occur\n",
    "        \n",
    "        outputs:\n",
    "        tform: a dict specifying the rotation, translation and scaling that maps X --> Y\n",
    "        \n",
    "        \"\"\"\n",
    "        matches, H, status = self.match_keypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh)\n",
    "        if len(matches) >= num_matches:\n",
    "            matchesA = []\n",
    "            matchesB = []\n",
    "            for match in matches:\n",
    "                matchesA.append(list(kpsA[match[1]]))\n",
    "                matchesB.append(list(kpsB[match[0]]))\n",
    "            matchesA = np.array(matchesA)\n",
    "            matchesB = np.array(matchesB)\n",
    "            tform = self.procrustes(matchesA, matchesB, True, False)\n",
    "            return tform\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def find_north(self, imageA_loc, imageB_loc, imageA_centerpoint, imageB_centerpoint, resize_ratio=6, alg=\"ORB\", ratio=0.75, reprojThresh=4, num_matches=4):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        imageA_Lat, imageA_Lon = imageA_centerpoint[0], imageA_centerpoint[1]\n",
    "        imageB_Lat, imageB_Lon = imageB_centerpoint[0], imageB_centerpoint[1]\n",
    "        \n",
    "        imageA = cv2.imread(imageA_loc)\n",
    "        imageB = cv2.imread(imageB_loc)\n",
    "        \n",
    "        #Resize large images to compute keypoints more quickly\n",
    "        imageA = cv2.resize(imageA, (int(imageA.shape[1]//resize_ratio),int(imageA.shape[0]//resize_ratio)))\n",
    "        imageB = cv2.resize(imageB, (int(imageB.shape[1]//resize_ratio),int(imageB.shape[0]//resize_ratio)))\n",
    "        \n",
    "        imageA_height, imageA_width = imageA.shape[0:2]\n",
    "        imageB_height, imageB_width = imageB.shape[0:2]\n",
    "        \n",
    "        #Detect and describe keypoints\n",
    "        kpsA, featuresA = self.detect_and_describe(imageA, alg)\n",
    "        kpsB, featuresB = self.detect_and_describe(imageB, alg)\n",
    "        \n",
    "        tform = self.match_2_images(kpsA, kpsB, featuresA, featuresB, num_matches, ratio, reprojThresh)\n",
    "        \n",
    "        headingA = Geodesic.WGS84.Inverse(imageB_Lat, imageB_Lon, imageA_Lat, imageA_Lon)['azi2']\n",
    "        headingB = Geodesic.WGS84.Inverse(imageB_Lat, imageB_Lon, imageA_Lat, imageA_Lon)['azi1']\n",
    "        \n",
    "        theta2 = math.degrees(math.atan(tform['translation'][1]/tform['translation'][0]))\n",
    "        \n",
    "        ansA = round(-(headingA) + theta2, 2)\n",
    "        ansB = round(-(headingB) + theta2 + math.degrees(math.asin(tform['rotation'][0,1])),2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if tform:\n",
    "            print('ImageA, true North: ', round(90-ansA,2))\n",
    "            print('ImageB, true North: ', round(90-ansB,2))\n",
    "            return tform\n",
    "        else:\n",
    "            raise Exception(\"Images could not be made into a collage.\")\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageA_loc = '../demo/find_north/images/c-300/raw/c-300_a-1.tif'\n",
    "imageB_loc = '../demo/find_north/images/c-300/raw/c-300_a-2.tif'\n",
    "FN = FindNorth()\n",
    "tform = FN.find_north(imageA_loc, imageB_loc, (34.8128,-118.8884), (34.8051,-118.8871), alg = \"SIFT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
